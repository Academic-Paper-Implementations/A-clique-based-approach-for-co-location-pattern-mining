{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9454ed2b",
   "metadata": {},
   "source": [
    "# Co-Location Pattern Mining Demo\n",
    "\n",
    "This notebook demonstrates two approaches for co-location pattern mining:\n",
    "1. **Synthetic Data Generation** - Generate test data with known patterns\n",
    "2. **CSV File Loading** - Load real data from CSV files\n",
    "\n",
    "Choose one of the methods below based on your needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f52f15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# For Jupyter Notebook, use the current working directory instead of __file__\n",
    "sys.path.insert(0, str(Path.cwd().parent))\n",
    "\n",
    "from colocation.synthetic import GeneratorParams, SyntheticSpatialGenerator\n",
    "from data.data import SpatialDataset\n",
    "from colocation.miner import CoLocationMiner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9f385",
   "metadata": {},
   "source": [
    "## Setup: Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "281bbac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## Method 1: Using Synthetic Data (Generated) - COMMENTED OUT\n",
    "\n",
    "## Generate synthetic spatial data with controlled parameters for testing algorithms.\n",
    "\n",
    "## **Note:** This section is commented out. Uncomment to use synthetic data instead of CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "91549013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Configure synthetic data generation parameters\n",
    "# params = GeneratorParams(\n",
    "#     P=10,        # Number of prevalent patterns\n",
    "#     I=200,       # Instances per feature\n",
    "#     D=5000.0,    # Space dimension\n",
    "#     F=10,        # Number of features\n",
    "#     Q=3,         # Pattern size\n",
    "#     m=10000,     # Total instances\n",
    "#     min_dist=50.0,  # Minimum distance threshold\n",
    "#     clumpy=1,    # Clumpiness level (1=sparse, 2-3=denser)\n",
    "# )\n",
    "# \n",
    "# # Generate synthetic dataset\n",
    "# gen = SyntheticSpatialGenerator(params, seed=42)\n",
    "# ds = gen.generate()\n",
    "# \n",
    "# print(f\"✓ Generated {len(ds.instances)} instances\")\n",
    "# print(f\"✓ Feature distribution: {ds.feature_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b945df63",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Method 2: Using CSV File (Real Data) - ACTIVE\n",
    "\n",
    "Load spatial data from a CSV file.\n",
    "\n",
    "**CSV Format Required:**\n",
    "```\n",
    "InstanceID,Feature,X,Y\n",
    "1,A,10,10\n",
    "1,B,12,11\n",
    "2,A,20,20\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c2711bc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded 30 instances from CSV\n",
      "✓ Feature distribution: {'A': 6, 'B': 6, 'C': 6, 'D': 6, 'E': 6}\n"
     ]
    }
   ],
   "source": [
    "# Load data from CSV file\n",
    "csv_path = Path.cwd().parent / \"data\" / \"sample_data.csv\"\n",
    "ds = SpatialDataset.from_csv(str(csv_path))\n",
    "\n",
    "print(f\"✓ Loaded {len(ds.instances)} instances from CSV\")\n",
    "print(f\"✓ Feature distribution: {ds.feature_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5472b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Run Co-Location Mining Algorithms\n",
    "\n",
    "Apply both IDS (Instance-Data-Structure) and NDS (Neighbor-Data-Structure) approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "06beddea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running co-location mining algorithms...\n",
      "  - Distance threshold: 5.0\n",
      "  - Prevalence threshold: 0.2\n",
      "✓ Mining completed!\n"
     ]
    }
   ],
   "source": [
    "# Configure mining parameters\n",
    "min_dist = 5.0   # Minimum neighbor distance (adjusted for CSV data scale)\n",
    "min_prev = 0.2   # Minimum prevalence threshold (0-1)\n",
    "\n",
    "# Initialize miner\n",
    "miner = CoLocationMiner(\n",
    "    dataset=ds,\n",
    "    min_dist=min_dist,\n",
    "    min_prev=min_prev\n",
    ")\n",
    "\n",
    "print(\"Running co-location mining algorithms...\")\n",
    "print(f\"  - Distance threshold: {min_dist}\")\n",
    "print(f\"  - Prevalence threshold: {min_prev}\")\n",
    "\n",
    "# Run both algorithms\n",
    "cliques_ids, prev_ids = miner.run_ids()  # IDS approach\n",
    "cliques_nds, prev_nds = miner.run_nds()  # NDS approach\n",
    "\n",
    "print(\"✓ Mining completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4051e5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cliques (IDS): 25\n",
      "\n",
      "Sample cliques (first 5):\n",
      "  1. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  2. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='B', index=1, x=12.0, y=11.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  3. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='C', index=1, x=11.0, y=13.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  4. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='B', index=1, x=12.0, y=11.0), Instance(feature='C', index=1, x=11.0, y=13.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  5. (Instance(feature='A', index=2, x=20.0, y=20.0), Instance(feature='C', index=2, x=19.0, y=21.0))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of cliques (IDS): {len(cliques_ids)}\")\n",
    "print(f\"\\nSample cliques (first 5):\")\n",
    "for i, clique in enumerate(list(cliques_ids)[:5], 1):\n",
    "    print(f\"  {i}. {clique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fabbacb8",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results: IDS Approach (Instance-Data-Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "697793e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of cliques (NDS): 17\n",
      "\n",
      "Sample cliques (first 5):\n",
      "  1. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='B', index=1, x=12.0, y=11.0), Instance(feature='C', index=1, x=11.0, y=13.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  2. (Instance(feature='A', index=1, x=10.0, y=10.0), Instance(feature='B', index=1, x=12.0, y=11.0), Instance(feature='D', index=1, x=13.0, y=12.0))\n",
      "  3. (Instance(feature='A', index=2, x=20.0, y=20.0), Instance(feature='B', index=2, x=21.0, y=22.0), Instance(feature='C', index=2, x=19.0, y=21.0))\n",
      "  4. (Instance(feature='A', index=3, x=40.0, y=40.0), Instance(feature='B', index=3, x=42.0, y=41.0), Instance(feature='C', index=3, x=41.0, y=39.0))\n",
      "  5. (Instance(feature='A', index=4, x=60.0, y=60.0), Instance(feature='B', index=4, x=61.0, y=62.0), Instance(feature='D', index=2, x=62.0, y=60.0))\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of cliques (NDS): {len(cliques_nds)}\")\n",
    "print(f\"\\nSample cliques (first 5):\")\n",
    "for i, clique in enumerate(list(cliques_nds)[:5], 1):\n",
    "    print(f\"  {i}. {clique}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a917b84",
   "metadata": {},
   "source": [
    "## Results: NDS Approach (Neighbor-Data-Structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "326833b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prevalent patterns (IDS): 17\n",
      "\n",
      "Sample prevalent patterns (first 5):\n",
      "  1. frozenset({'C'})\n",
      "  2. frozenset({'B'})\n",
      "  3. frozenset({'C', 'B'})\n",
      "  4. frozenset({'A'})\n",
      "  5. frozenset({'C', 'A'})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of prevalent patterns (IDS): {len(prev_ids)}\")\n",
    "print(f\"\\nSample prevalent patterns (first 5):\")\n",
    "for i, pattern in enumerate(list(prev_ids)[:5], 1):\n",
    "    print(f\"  {i}. {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7701cd58",
   "metadata": {},
   "source": [
    "## Prevalent Patterns\n",
    "\n",
    "Patterns that meet the minimum prevalence threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10111ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of prevalent patterns (NDS): 17\n",
      "\n",
      "Sample prevalent patterns (first 5):\n",
      "  1. frozenset({'C'})\n",
      "  2. frozenset({'B'})\n",
      "  3. frozenset({'C', 'B'})\n",
      "  4. frozenset({'A'})\n",
      "  5. frozenset({'C', 'A'})\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of prevalent patterns (NDS): {len(prev_nds)}\")\n",
    "print(f\"\\nSample prevalent patterns (first 5):\")\n",
    "for i, pattern in enumerate(list(prev_nds)[:5], 1):\n",
    "    print(f\"  {i}. {pattern}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb26796e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "**Key Differences:**\n",
    "- **Synthetic Data**: Generated with controlled parameters, useful for testing and validation\n",
    "- **CSV Data**: Real-world data, requires proper formatting (InstanceID, Feature, X, Y columns)\n",
    "\n",
    "**How to Switch:**\n",
    "1. For synthetic data: Use Method 1 (current default)\n",
    "2. For CSV data: Comment out Method 1 cells, uncomment Method 2 cell\n",
    "3. Adjust `min_dist` parameter based on your data scale"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dhiep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
